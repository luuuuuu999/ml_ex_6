{"metadata":{"kernelspec":{"name":"","display_name":""},"language_info":{"name":"python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"20d99f03","cell_type":"markdown","source":"# **Ensemble methods**","metadata":{}},{"id":"a36f6509","cell_type":"markdown","source":"# Iris Dataset\n\nIl dataset Iris è un classico dataset nell'apprendimento automatico e nella statistica, introdotto da Ronald Fisher nel 1936. È comunemente utilizzato per attività di classificazione e clustering.\n\n## Caratteristiche e Struttura\n- **Campioni**: 150 campioni di fiori iris.\n- **Features**:\n  - Lunghezza del sepalo (cm)\n  - Larghezza del sepalo (cm)\n  - Lunghezza del petalo (cm)\n  - Larghezza del petalo (cm)\n- **Classi (Etichette Target)**:\n  - *Iris-setosa*\n  - *Iris-versicolor*\n  - *Iris-virginica*\n\nOgni classe è rappresentata da 50 campioni.\n\n## Caratteristiche Principali\n- **Balanced Dataset**: Ogni classe contiene lo stesso numero di campioni.\n- **Perfect for Beginners**: a sua semplicità e struttura ben definita lo rendono perfetto per scopi didattici.\n- **Separable Classes**:\n  - *Iris-setosa* è linearmente separabile dalle altre due classi.\n  - *Iris-versicolor* e *Iris-virginica* sono più difficili da separare tra loro.\n\n\n# Iris Dataset Classes\n\n<table>\n    <tr>\n        <th>Iris Setosa</th>\n        <th>Iris Versicolor</th>\n        <th>Iris Virginica</th>\n    </tr>\n    <tr>\n        <td><img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSFn-u9Lagrv8pV4zJ8Z1cEqXNL_uo39CrL6A&s\" alt=\"Iris setosa\" width=\"300\" height=\"300\"></td>\n        <td><img src=\"https://encrypted-tbn3.gstatic.com/images?q=tbn:ANd9GcSJqxUtJiLfMX5aIoyPTPz7rMdjxgWagMlBzt0QbfATKzRqH4XnMMDN5aBrU1FvRt19jkHMOrIefjywQlDg9rOeKC6JbA72Wf--jqHD-g\" alt=\"Iris versicolor\" alt=\"Iris versicolor\" width=\"300\" height=\"300\"></td>\n        <td><img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSQbTwTLA7_7SeTE3B1QOKw0TlB8Rp6NU7vyg&s\" alt=\"Iris virginica\" width=\"300\" height=\"300\"></td>\n    </tr>\n</table>\n\n","metadata":{}},{"id":"891a70d4","cell_type":"markdown","source":"# `plot_decision_boundary` Function\n\nLa funzione `plot_decision_boundary` permette di visualizzare i margini decisionali di un classificatore. In questo modo è possibile visualizzare come il modello distingue le feature di campioni assegnati a classi diverse.\n\n1. **Parametri**:\n    - `clf`: Il classificatore allenato che ha il metodo `predict`.\n    - `X`: La matrice dei dati di input, per cui si assume una dimensione 2D per la visualizzazione.\n    - `y`: Le labels corrispondenti ai dati `X`.\n\n2. **Output**:\n    - Un grafico 2D contenente:\n        - **Features**: `Feature 1` e `Feature 2` lungo l' asse x e y (se è stata usata PCA, queste sono le 2 componenti).\n        - **Regioni decisionali**: Colori diversi indicano regioni classificate come labels diverse.\n        - **Camponi**: I punti del dataset (`X`) sono sovrapposti alle regioni e colorati in base alla loro label corretta (`y`).\n\n### **Sintassi**\n\n```python\nplot_decision_boundary(trained_model, X_data, y_data)\n```\n","metadata":{}},{"id":"0062646f","cell_type":"code","source":"# Helper function to create the plot\n# and visualize the decision boundary\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef plot_decision_boundary(clf, X, y):\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500), np.linspace(y_min, y_max, 500))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n\n    plt.figure(figsize=(10, 6))\n    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\n    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', cmap=plt.cm.RdYlBu)\n    plt.title(\"Decision Boundary Visualization\")\n    plt.xlabel(\"Feature 1\")\n    plt.ylabel(\"Feature 2\")\n    plt.show()","metadata":{},"outputs":[],"execution_count":null},{"id":"5184bd53","cell_type":"markdown","source":"## Funzione `train_test_split()` :\n\n---\n\n## **train_test_split()**\n\nLa funzione `train_test_split()` è parte del modulo `sklearn.model_selection`. Viene utilizzata per dividere un dataset in training e test set.\n\n\n### **Esempio**:\n```python\nfrom sklearn.model_selection import train_test_split\n\n# Splitting the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(f\"Training data: {len(X_train)} samples\")\nprint(f\"Testing data: {len(X_test)} samples\")\n```\n","metadata":{}},{"id":"8ccb9a84","cell_type":"markdown","source":"# **Esercizio 1: Classifichiamo il dataset Iris con un DecisionTree**\n\nEseguire tutti gli step di preparazione per l' allenamento di un DecisionTree. Per lo split dai dati in training e test utilizzare:\n\n- `test_size` = `0.3`\n\n- `random_state` = `42`","metadata":{}},{"id":"ded18e38","cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{},"outputs":[],"execution_count":null},{"id":"1452eca5","cell_type":"markdown","source":"# **Eesercizio 2: Implementare ensamble methods**\n\nUna volta allenato il DecisionTree nell' esercizio 1, vogliamo applicarvi i metodi di ensamble. Nello specifico andremo a implementare:\n\n* **AdaBoost**\n* **Bagging**\n* **Random Forest**\n\nDi seguito vediamo la sintassi di ognuno di questi.","metadata":{}},{"id":"222b2e08","cell_type":"markdown","source":"## 1. **AdaBoostClassifier**: \n\nL' `AdaBoostClassifier` crea un insieme di alberi decisionali deboli. Assegna un peso a ciascun albero e li combina per formare un modello più robusto.\n\n### Example:\n```python\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# Train the model\nada_clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1), n_estimators=50, random_state=42)\nada_clf.fit(X_train, y_train)\n\n# Get predictions\npredictions = ada_clf.predict(X_test)\n\n```\n\n---\n\n## 2. **RandomForestClassifier**: \n\nIl `RandomForestClassifier` costruisce più alberi in parallelo e combina i loro risultati per migliorare l'accuratezza.\n\n### Example:\n```python\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Train the model\nrf_clf = RandomForestClassifier(n_estimators=10, max_depth=3, random_state=42)\nrf_clf.fit(X_train, y_train)\n\n# Get predictions\npredictions = rf_clf.predict(X_test)\n\n```\n\n#### **N.B. RandomForest non richiede come argomento il classificatore, differentemente dagli altri metodi.**\n\n---\n\n## 3. **BaggingClassifier**:\n\nIl `BaggingClassifier` combina molteplici modelli base (come DecisionTree) utilizzando la tecnica del bootstrapping per ridurre la varianza.\n\n### Example:\n```python\nfrom sklearn.ensemble import BaggingClassifier\n\n# Train the model\nbag_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=10, random_state=42)\nbag_clf.fit(X_train, y_train)\n\n# Get predictions\npredictions = bag_clf.predict(X_test)\n\n```\n","metadata":{}},{"id":"c6c29229","cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier, BaggingClassifier\n\n# Creare gli oggetti ensemble\n\nbag_clf = BaggingClassifier(base_estimator=DecisionTreeClassifier(), \nclf = AdaBoostClassifier ( estimator = base_clf , n_estimators =50 , random_state =42)\nrf_clf = RandomForestClassifier(n_estimators=10, max_depth=3, random_state=42)\n\n# Allenare i modelli\n\nbag_clf.fit(X_train, y_train)\nclf.fit(X_train, y_train)\nrf_clf.fit(X_train, y_train)\n\n# Valutare e stampare le prestazioni dei modelli\n\nbag_clf_pred = bag_clf.predict(X_test)\nclf_pred = clf.predict(X_test)\nrf_clf_pred = rf_clf.predict(X_test)\n\nbag_clf_accuracy = accuracy.score(y_test, bag_clf_pred)\nclf_accuracy = accuracy.score(y_test, clf_pred)\nrf_clf_accuracy = accuracy.score(y_test, rf_clf_pred)\n\nprint (f\" Accuracy del modello Bagging : { bag_clf_accuracy :.2 f}\")\nprint (f\" Accuracy del modello AdaBoost : { clf_accuracy :.2 f}\")\nprint (f\" Accuracy del modello Random Forest : { rf_clf_accuracy :.2 f}\")\n","metadata":{},"outputs":[],"execution_count":null},{"id":"78e9035c","cell_type":"markdown","source":"# Funzioni utili per Ensemble Models\n\n---\n\n## **1. Hard Voting tra tre classificatori**\n\n### **Descrizione**:\nCombina le predizioni da tre classificatori selezionando la casse più votata per ogni campione.\n\n### **Parametri**:\n- `pred1` (numpy array): Predizioni del classificatore 1.\n- `pred2` (numpy array): Predizioni del classificatore 2.\n- `pred3` (numpy array): Predizioni del classificatore 3.\n\n### **Output**:\n- Restituisce un numpy array conenente la classe più votata per ogni campione.\n\n### **Sintassi**:\n```python\nvoted = hard_voting(pred1_test, pred2_test, pred3_test)\nprint(\"Hard Voting Predictions:\", voted)\n```\n\n---\n\n## **2. Allineare predizioni a più classi**\n\n### **Descrizione**:\nAllinea le probabilità in modo da rendere compatibili diversi subsets quando vengono combinati.\n\n### **Parametri**:\n- `pred` (numpy array): Probabilità predette da un classificatore.\n- `classes_present` (list): Classi conosciute dal classificatore.\n- `n_classes` (int, optional): Numero totale di classe, default è 3.\n\n### **Output**:\n- Ritorna un numpy array con le proabilità allineate tra tutte le classi\n- Returns a numpy array with probabilities aligned across all classes.\n\n### **Sintassi**:\n```python\naligned_probs = align_predictions(pred, [0, 1], n_classes=3)\nprint(\"Aligned Probabilities:\", aligned_probs)\n\nMatrice originale [[0.7, 0.3], [0.4, 0.6]]\ndiventa\nMatrice allineata [[0.7, 0, 0.3], [0.4, 0, 0.6]]\n\n```\n\n---\n\n## **3. Plot Decision Boundary**\n\n### **Descrizione**:\nVisualizza i margini decisionali per vari ensamble methods, inclusi `expert1`, `expert2`, `expert3`, `base`, `soft_voting`, `hard_voting`, e `gating`.\n\n### **Parametri**:\n- `X` (numpy array): Dati per la visualizzazione (2D).\n- `y` (numpy array): Lables originali.\n- `clf1`, `clf2`, `clf3` (classifiers, optional): Esperti usati per calcolare prediction.\n- `base` (classifier, optional): Classificatore base.\n- `mode` (string): Metodo di ensamble per la visualizzazione. I valori possibili sono: `\"expert1\"`, `\"expert2\"`, `\"expert3\"`, `\"base\"`, `\"soft_voting\"`, `\"hard_voting\"`, `\"gating\"`.\n\n\n### **Output**:\n- Displays a decision boundary plot for the selected mode.\n\n### **Usage**:\n```python\nplot_decision_boundary(x_test, y_test, expert1=expert1, clf2=expert2, clf3=expert3, base=base_network, mode=\"soft_voting\")\n```\n","metadata":{}},{"id":"9f27abad","cell_type":"markdown","source":"# **Esercizio 3: Implementare Mixture of Experts**\n\nNel seguente esercizio vogliamo implementare un meccanismo di Mixture of Experts. Poichè in iris sono presenti 3 classi, vogliamo allenare 3 classificatori (cioè 3 esperi), rispettivamente:\n\n- esperto 1: riconosce tra la classe 0 e la classe 1.\n- esperto 2: riconosce tra la classe 0 e la classe 2.\n- esperto 3: riconosce tra la classe 1 e la classe 2.\n\nInfine utilizzare la funzione `plot_modes` per plottare le diverse modalità.","metadata":{}},{"id":"45ffeeeb","cell_type":"code","source":"def hard_voting(pred1, pred2, pred3):\n    combined = np.vstack([pred1, pred2, pred3]).T\n    voted = []\n    for sample in combined:\n        counts = np.bincount(sample)\n        most_common_label = counts.argmax()\n        voted.append(most_common_label)\n    return np.array(voted)\n\ndef align_predictions(pred, classes_present, n_classes=3):\n    aligned = np.zeros((len(pred), n_classes))\n    for idx, class_label in enumerate(classes_present):\n        aligned[:, class_label] = pred[:, idx]\n    return aligned\n\ndef plot_modes(X, y, clf1=None, clf2=None, clf3=None, base=None, mode=\"expert1\"):\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500),\n                         np.linspace(y_min, y_max, 500))\n    grid = np.c_[xx.ravel(), yy.ravel()]\n\n    if mode.startswith(\"expert\"):\n        expert_map = {\n            \"expert1\": (clf1, [0, 1]),\n            \"expert2\": (clf2, [0, 2]),\n            \"expert3\": (clf3, [1, 2]),\n        }\n        clf, classes = expert_map[mode]\n        Z = clf.predict(grid).reshape(xx.shape)\n        title = f\"Decision Boundary - {mode.capitalize()}\"\n\n    elif mode == \"base\":\n        Z = base.predict(grid).reshape(xx.shape)\n        title = \"Decision Boundary - Base Network\"\n\n    elif mode == \"soft_voting\":\n        Z1 = align_predictions(clf1.predict_proba(grid), [0, 1])\n        Z2 = align_predictions(clf2.predict_proba(grid), [0, 2])\n        Z3 = align_predictions(clf3.predict_proba(grid), [1, 2])\n        Z = (Z1 + Z2 + Z3).argmax(axis=1).reshape(xx.shape)\n        title = \"Decision Boundary - Soft Voting\"\n\n    elif mode == \"hard_voting\":\n        Z1 = clf1.predict(grid)\n        Z2 = clf2.predict(grid)\n        Z3 = clf3.predict(grid)\n        Z = np.array([Z1, Z2, Z3])\n        Z = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=Z)\n        Z = Z.reshape(xx.shape)\n        title = \"Decision Boundary - Hard Voting\"\n\n    elif mode == \"gating\":\n        gating_weights = base.predict_proba(grid)\n        gating_weights /= gating_weights.sum(axis=1, keepdims=True)\n        Z1 = align_predictions(clf1.predict_proba(grid), [0, 1])\n        Z2 = align_predictions(clf2.predict_proba(grid), [0, 2])\n        Z3 = align_predictions(clf3.predict_proba(grid), [1, 2])\n        Z = gating_weights * Z1 + gating_weights * Z2 + gating_weights * Z3\n        Z = Z.argmax(axis=1).reshape(xx.shape)\n        title = \"Decision Boundary - Mixture of Experts (Gating)\"\n\n    else:\n        raise ValueError(f\"Unknown mode: {mode}\")\n\n    plt.figure(figsize=(10, 6))\n    plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\n    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolor='k', cmap=plt.cm.RdYlBu)\n    plt.title(title)\n    plt.xlabel(\"Feature 1\")\n    plt.ylabel(\"Feature 2\")\n    plt.show()","metadata":{},"outputs":[],"execution_count":null},{"id":"610f8315","cell_type":"code","source":"# Caricare il dataset Iris\n\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Split del dataset in training e test set\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\n\n# Applicare lo scaling e PCA\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\npca = PCA(n_components=2)\nX_train_pca = pca.fit_transform(X_train_scaled)\nX_test_pca = pca.transform(X_test_scaled)\n\n# Creare 3 modelli di Decision Tree, ognuno dovrà essere esperto in una coppia di classi\n# N.B. ogni esperto dovrà essere allenato su un sottoinsieme contentente solo \n# le classi di competenza\n\nexpert1 = DecisionTreeClassifier()\nexpert2 = DecisionTreeClassifier()\nexpert3 = DecisionTreeClassifier()\n\nexpert_labels = np . digitize ( X_train , bins =[ -1 , 1]) # 0, 1, 2 ( per i tre esperti )\n\nexpert1 . fit ( X_train [ expert_labels . flatten () == 0] , y_train [ expert_labels . flatten () ==\n1])\nexpert2 . fit ( X_train [ expert_labels . flatten () == 0] , y_train [ expert_labels . flatten () ==\n2])\nexpert3 . fit ( X_train [ expert_labels . flatten () == 2] , y_train [ expert_labels . flatten () ==\n3])\n\n# Creare un modello di Decision Tree che funge da base network\n# N.B. il base network dovrà essere allenato su tutte le classi, sarà la nostra gating network\n\nbase_network = DecisionTreeClassifier(random_state=42)\nbase_network.fit(X_train_pca, y_train)\n\n# Estrarre le predizione di ogni esperto sul test set.\n# N.B. Vogliamo le probabilità di appartenenza a ciascuna classe, quindi useremo `predict_proba`. Inoltre \n# ogni esperto riporterà solo le classi di competenza, quindi dovremo allineare le predizioni.\nprobs_exp1 = align_predictions(expert1.predict_proba(X_test_pca), [0, 1])\nprobs_exp2 = align_predictions(expert2.predict_proba(X_test_pca), [0, 2])\nprobs_exp3 = align_predictions(expert3.predict_proba(X_test_pca), [1, 2])\n\n# Estrarre le predizioni del base network sul test set\n\nprobs_base = base_network.predict_proba(X_test_pca)\n\n# Usiamo hard voting per combinare le predizioni degli esperti. \n# N.B. hard voting significa che ogni esperto vota per la sua classe di competenza e il voto più comune\n# vince.\npred_exp1 = expert1.predict(X_test_pca)\npred_exp2 = expert2.predict(X_test_pca)\npred_exp3 = expert3.predict(X_test_pca)\n\nhard_voting= hard_voting(pred_exp1, pred_exp2, pred_exp3)\nprint(\"Hard Voting Predictions:\", voted)\n\n# Usiamo soft voting per combinare le predizioni degli esperti.\n# N.B. soft voting significa che il voto di ogni esperto ha lo stesso peso.\n\nsoft_vote_probs = probs_exp1 + probs_exp2 + probs_exp3\nsoft_vote_pred = soft_vote_probs.argmax(axis=1)\n\n\n# Usiamo gating network per combinare le predizioni degli esperti.\n# N.B. la gating network calcola le probabilità di appartenenza a ciascuna classe e le usa per pesare\n# le predizioni degli esperti.\n\ngating_weights = probs_base / probs_base.sum(axis=1, keepdims=True)\ngating_probs = (gating_weights * probs_exp1) + (gating_weights * probs_exp2) + (gating_weights * probs_exp3)\ngating_pred = gating_probs.argmax(axis=1)\n\nmodes = [\"expert1\", \"expert2\", \"expert3\", \"base\", \"soft_voting\", \"hard_voting\", \"gating\"]\n\n# Plottare le decision boundaries per ogni modalità.\n\nfor mode in modes:\n    plot_modes(X_test_pca, y_test, clf1=expert1, clf2=expert2, clf3=expert3, base=base_network, mode=mode)\n","metadata":{},"outputs":[],"execution_count":null}]}